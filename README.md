# Phishing Neural Network Using Logarithmic Regression

Based on a neural network:

*Logistic Regression:* Uses Logistic Function, Neural Network, Sigmoid


Need text segmentation to binary processing,

Three Steps:

*Binary Classifier*

*Gemini API integration*

*Launch UI*


Order of Neural Network:

Raw text  â”€â”€â–º preprocessing (clean, tokenize) â”€â”€â–º feature extraction â”€â”€â–º numeric vector x
x â”€â”€â–º logistic regression â”€â”€â–º probability p â”€â”€â–º decision (phishing vs ham)

# ğŸ“§ Phishingâ€‘Detection Binary Classifier  

A lightweight,â€¯**explainable** model that decides whether a short text message is **phishing** (`spam`) or **legitimate** (`ham`).  
The pipeline combines classic **TFâ€‘IDF** representations with a few **handâ€‘crafted binary features** (URL flag, domain whitelist, keyword counters, firstâ€‘person pronoun flag) and a **logisticâ€‘regression** classifier.

---

## Table of Contents
1. [Problem statement](#problem-statement)  
2. [Data format](#data-format)  
3. [Mathematical model](#mathematical-model)  
4. [Feature engineering](#feature-engineering)  
   - [1ï¸âƒ£ Wordâ€‘level TFâ€‘IDF](#1-wordâ€‘level-tfidf)  
   - [2ï¸âƒ£ Characterâ€‘level TFâ€‘IDF](#2-characterâ€‘level-tfidf)  
   - [3ï¸âƒ£ URL flag](#3-urlâ€‘flag)  
   - [4ï¸âƒ£ Domain whitelist](#4-domain-whitelist)  
   - [5ï¸âƒ£ Keyword counter](#5-keywordâ€‘counter)  
   - [6ï¸âƒ£ Firstâ€‘person pronoun flag](#6-firstâ€‘personâ€‘pronounâ€‘flag)  
5. [Training pipeline (code)](#training-pipeline-code)  
6. [Evaluation metrics](#evaluation-metrics)  
7. [Inference (code)](#inference-code)  
8. [How to run the code](#how-to-run-the-code)  
9. [Extending / customizing](#extending-customizing)  

---

## Problem statement <a name="problem-statement"></a>

Given a raw text message  

$$\mathbf{s}= (c_1,c_2,\dots,c_L)\in\mathcal{V}^L$$  

(where $\mathcal{V}$ is the character alphabet), predict a binary label  

$$y\in\{0,1\},\qquad
y=1\;\Longleftrightarrow\;\text{â€œphishingâ€},\;
y=0\;\Longleftrightarrow\;\text{â€œlegitimateâ€}.$$

The model is **linear** in a highâ€‘dimensional sparse feature space $\mathbf{x}\in\mathbb{R}^d$.

---

## Data format <a name="data-format"></a>

| Column | Description |
|--------|-------------|
| `label` | `"ham"`â€¯/â€¯`"spam"` (or `"legitimate"`â€¯/â€¯`"phishing"`). |
| `text`  | Raw message string, **no header row**. |
| **Location** | `data/spam.csv` (relative to the repository root). |

The script automatically maps the textual labels to integers (`0`â€¯=â€¯ham, `1`â€¯=â€¯spam).

---

## Mathematical model <a name="mathematical-model"></a>

### 1ï¸âƒ£ Feature vector  

All preprocessing steps produce a **single sparse vector**  

$$
\mathbf{x}= \big[\,\mathbf{x}^{\text{word}};\;
\mathbf{x}^{\text{char}};\;
x^{\text{url}};\;
\mathbf{x}^{\text{kw}};\;
x^{\text{wl}};\;
x^{\text{fp}}\big]\in\mathbb{R}^{d}
$$  

where  

* $\mathbf{x}^{\text{word}}$ â€“ TFâ€‘IDF on unigramsâ€¯+â€¯bigrams,  
* $\mathbf{x}^{\text{char}}$ â€“ TFâ€‘IDF on character 3â€‘5â€‘grams,  
* $x^{\text{url}}\in\{0,1\}$ â€“ presence of any URL,  
* $\mathbf{x}^{\text{kw}}$ â€“ counts of a curated list ofâ€¯~120 phishingâ€‘related keywords,  
* $x^{\text{wl}}\in\{0,1\}$ â€“ domain belongs to a trusted **whitelist**,  
* $x^{\text{fp}}\in\{0,1\}$ â€“ contains a firstâ€‘person pronoun (`I`, `my`, `we`, â€¦).  

All components are concatenated columnâ€‘wise, producing a sparse matrix $X\in\mathbb{R}^{n\times d}$ for the whole dataset.

### 2ï¸âƒ£ Logistic regression  

We learn a weight vector $\mathbf{w}$ and bias $b$ by minimising the regularised logâ€‘loss  

$$
\min_{\mathbf{w},b}\;
\frac{1}{n}\sum_{i=1}^{n}
\bigl[ -y_i\log\sigma(z_i) - (1-y_i)\log\bigl(1-\sigma(z_i)\bigr)\bigr]
+ \lambda\|\mathbf{w}\|_2^{2},
$$  

with  

$$
z_i = \mathbf{w}^{\!\top}\mathbf{x}_i + b,
\qquad
\sigma(z)=\frac{1}{1+e^{-z}}.
$$  

`scikitâ€‘learn`â€™s `LogisticRegression` (solver **lbfgs**) solves exactly this problem.  
We set `class_weight='balanced'` so that the loss is automatically reâ€‘weighted to counter any class imbalance.

The **output probability** for a new message $\mathbf{s}$ is  

$$
\hat{p}= \sigma\bigl(\mathbf{w}^{\!\top}\mathbf{x}(\mathbf{s}) + b\bigr).
$$

A hard decision is obtained by thresholding  

$$
\hat{y}= \mathbf{1}\bigl(\hat{p}\ge\tau\bigr),
$$  

with the default threshold $\tau=0.5$ (you can tune it).

---

## Feature engineering <a name="feature-engineering"></a>

Below each feature is described **conceptually** and by the concrete **Python implementation**.

### 1ï¸âƒ£ Wordâ€‘level TFâ€‘IDF <a name="1-wordâ€‘level-tfidf"></a>

| Concept | Code |
|---------|------|
| Unigrams + bigrams on the **cleaned** text. | ```python<br>TfidfVectorizer(ngram_range=(1,2), max_features=None, min_df=1,<br>                 sublinear_tf=True, stop_words=None)``` |
| TFâ€‘IDF term weight: $\displaystyle \text{tfidf}(t,d)=\text{tf}(t,d)\cdot\log\frac{N}{\text{df}(t)}$ (subâ€‘linear). | Handled internally by `scikitâ€‘learn`. |

### 2ï¸âƒ£ Characterâ€‘level TFâ€‘IDF <a name="2-characterâ€‘level-tfidf"></a>

Captures **obfuscated words** and **URL fragments**.

| Concept | Code |
|---------|------|
| Character 3â€‘ to 5â€‘grams, wordâ€‘boundary aware (`analyzer='char_wb'`). | ```python<br>TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5),<br>                 min_df=1, sublinear_tf=True, max_features=5000)``` |
| Same TFâ€‘IDF weighting as above. | `scikitâ€‘learn` does the math. |

### 3ï¸âƒ£ URL flag <a name="3-urlâ€‘flag"></a>

| Concept | Code |
|---------|------|
| Binary column `1` if any substring matches `https?://\S+` or `www.\S+`. | ```python<br>class UrlFlagTransformer(BaseEstimator, TransformerMixin):<br>    def __init__(self):<br>        self.regex = re.compile(r"https?://\S+|www\.\S+")<br>    def transform(self, X):<br>        flags = [1 if self.regex.search(txt) else 0 for txt in X]<br>        return sparse.csr_matrix(np.array(flags).reshape(-1,1))``` |

### 4ï¸âƒ£ Domain whitelist <a name="4-domain-whitelist"></a>

If the URL belongs to a **trusted domain** (e.g. `amazon.com`, `paypal.com`) we set a **negative indicator**.

| Concept | Code |
|---------|------|
| Extract the first URL, normalise the domain, check membership in a hardâ€‘coded whitelist. | ```python<br>class DomainWhitelistTransformer(BaseEstimator, TransformerMixin):<br>    def __init__(self, whitelist=None):<br>        self.whitelist = set(w.lower() for w in (whitelist or [<br>            "amazon.com","paypal.com","google.com","facebook.com",<br>            "apple.com","microsoft.com","ebay.com","netflix.com",<br>            "github.com","stackoverflow.com","nytimes.com"<br>        ]))<br>    def transform(self, X):<br>        pattern = re.compile(r"https?://([^/]+)")<br>        flags = []<br>        for txt in X:<br>            m = pattern.search(txt)<br>            if m:<br>                domain = m.group(1).lower().lstrip("www.")<br>                flags.append(1 if domain in self.whitelist else 0)<br>            else:<br>                flags.append(0)<br>        return sparse.csr_matrix(np.array(flags).reshape(-1,1))``` |

### 5ï¸âƒ£ Keyword counter <a name="5-keyword-counter"></a>

| Concept | Code |
|---------|------|
| 120+ manually curated phishingâ€‘related words/phrases (e.g. *â€œverifyâ€, â€œaccountâ€, â€œurgentâ€, â€œ$â€*). | ```python<br>class KeywordFlagTransformer(BaseEstimator, TransformerMixin):<br>    def __init__(self, keywords=None):<br>        self.keywords = [...]   # full list from the file<br>    def transform(self, X):<br>        rows = []<br>        for txt in X:<br>            txt_lc = txt.lower()<br>            rows.append([txt_lc.count(kw) for kw in self.keywords])<br>        return sparse.csr_matrix(rows)``` |

### 6ï¸âƒ£ Firstâ€‘person pronoun flag <a name="6-first-person-pronoun-flag"></a>

| Concept | Code |
|---------|------|
| Binary column `1` if any token belongs to `{i, me, my, we, our}`. | ```python<br>class FirstPersonPronounTransformer(BaseEstimator, TransformerMixin):<br>    def __init__(self):<br>        self.pronouns = {"i","me","my","we","our"}<br>    def transform(self, X):<br>        flags = [1 if set(txt.lower().split()) & self.pronouns else 0 for txt in X]<br>        return sparse.csr_matrix(np.array(flags).reshape(-1,1))``` |

All four custom transformers are **stateless** (no learned parameters), so they can be **pickled** together with the pipeline.

---

## Training pipeline (code) <a name="training-pipeline-code"></a>

```python
# train_spam.py
import re, pandas as pd
from pathlib import Path
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, FeatureUnion
from feature_transformers import (
    UrlFlagTransformer,
    KeywordFlagTransformer,
    DomainWhitelistTransformer,
    FirstPersonPronounTransformer,
)

# -----------------------------------------------------------------
# 0ï¸âƒ£ Paths
# -----------------------------------------------------------------
CSV_PATH   = Path("data/spam.csv")
MODEL_PATH = Path("model/spam_detector.pkl")
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)

# -----------------------------------------------------------------
# 1ï¸âƒ£ Load & clean data
# -----------------------------------------------------------------
df = pd.read_csv(
    CSV_PATH, header=None, usecols=[0,1],
    names=["label","text"], encoding="utf-8", engine="python")
df.dropna(subset=["label","text"], inplace=True)

label_map = {"ham":0, "legitimate":0, "spam":1, "phishing":1}
df["label"] = df["label"].map(label_map).astype(int)

def clean_text(t):
    return re.sub(r"\s+", " ", t.replace("\uFFFD","")).strip()
df["clean"] = df["text"].apply(clean_text)

# -----------------------------------------------------------------
# 2ï¸âƒ£ Train / test split
# -----------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    df["clean"], df["label"], test_size=0.2,
    stratify=df["label"], random_state=42)

# -----------------------------------------------------------------
# 3ï¸âƒ£ FeatureUnion + LogisticRegression
# -----------------------------------------------------------------
pipeline = Pipeline([
    ("features", FeatureUnion([
        ("tfidf_word",   TfidfVectorizer(ngram_range=(1,2),
                                         max_features=None,
                                         min_df=1,
                                         sublinear_tf=True,
                                         stop_words=None)),
        ("tfidf_char",   TfidfVectorizer(analyzer="char_wb",
                                         ngram_range=(3,5),
                                         min_df=1,
                                         sublinear_tf=True,
                                         max_features=5000)),
        ("url_flag",     UrlFlagTransformer()),
        ("keyword_flag", KeywordFlagTransformer()),
        ("whitelist",    DomainWhitelistTransformer()),
        ("first_person", FirstPersonPronounTransformer()),
    ])),
    ("clf", LogisticRegression(max_iter=3000,
                               class_weight="balanced",
                               solver="lbfgs"))
])

pipeline.fit(X_train, y_train)

# -----------------------------------------------------------------
# 4ï¸âƒ£ Evaluation
# -----------------------------------------------------------------
from sklearn.metrics import classification_report, roc_auc_score
y_prob = pipeline.predict_proba(X_test)[:,1]
y_pred = (y_prob >= 0.5).astype(int)

print("\n=== Classification report ===")
print(classification_report(y_test, y_pred,
                            target_names=["ham","phishing"]))
print("ROCâ€‘AUC :", roc_auc_score(y_test, y_prob))

# -----------------------------------------------------------------
# 5ï¸âƒ£ Save the whole pipeline
# -----------------------------------------------------------------
joblib.dump(pipeline, MODEL_PATH)
print(f"\nâœ… Model saved to {MODEL_PATH}")
